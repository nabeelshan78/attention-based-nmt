{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"dataset/raw/eng_fra.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "    for line in infile:\n",
    "        lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Examples: 237838\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Examples:\", len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\\n',\n",
       " 'Go.\\tMarche.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)\\n']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"I went drinking with one of my boyfriend\\'s friends, and now he\\'s furious at me.\" \"Was this friend a guy or a girl?\" \"A guy, obviously. Why would I go drinking with his female friends?\" \"Yeah, you\\'re right.\" \"His name is Tom. He\\'s really hot, and I really want to go drinking with him again.\"\\t«\\xa0Je suis allée boire avec un ami de mon compagnon, et voilà qu\\'il est furieux contre moi.\\xa0» «\\xa0Était-ce un gars ou une fille\\xa0?\\xa0» «\\xa0Un gars, bien évidemment. Pourquoi irais-je boire avec ses amies\\xa0?\\xa0» «\\xa0Ouais, ça se comprend.\\xa0» «\\xa0Il s\\'appelle Tom. Il est trop canon, et j\\'ai tellement envie d\\'aller prendre un verre avec lui à nouveau.\\xa0»\\tCC-BY 2.0 (France) Attribution: tatoeba.org #9821215 (DJ_Saidez) & #11726136 (Micsmithel)\\n'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 lines:\n",
      "Go.\tVa !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\n",
      "\n",
      "Go.\tMarche.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)\n",
      "\n",
      "Go.\tEn route !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8267435 (felix63)\n",
      "\n",
      "Go.\tBouge !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #9022935 (Micsmithel)\n",
      "\n",
      "Hi.\tSalut !\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)\n",
      "\n",
      "Hi.\tSalut.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)\n",
      "\n",
      "Run!\tCours !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906331 (sacredceltic)\n",
      "\n",
      "Run!\tCourez !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906332 (sacredceltic)\n",
      "\n",
      "Run!\tPrenez vos jambes à vos cous !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #2077449 (sacredceltic)\n",
      "\n",
      "Run!\tFile !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #2077454 (sacredceltic)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('First 10 lines:')\n",
    "for line in lines[:10]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 2 lines:\n",
      "It may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort. However, if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning, we might be able to minimize errors.\tIl est peut-être impossible d'obtenir un Corpus complètement dénué de fautes, étant donnée la nature de ce type d'entreprise collaborative. Cependant, si nous encourageons les membres à produire des phrases dans leurs propres langues plutôt que d'expérimenter dans les langues qu'ils apprennent, nous pourrions être en mesure de réduire les erreurs.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2024159 (CK) & #2024564 (sacredceltic)\n",
      "\n",
      "\"I went drinking with one of my boyfriend's friends, and now he's furious at me.\" \"Was this friend a guy or a girl?\" \"A guy, obviously. Why would I go drinking with his female friends?\" \"Yeah, you're right.\" \"His name is Tom. He's really hot, and I really want to go drinking with him again.\"\t« Je suis allée boire avec un ami de mon compagnon, et voilà qu'il est furieux contre moi. » « Était-ce un gars ou une fille ? » « Un gars, bien évidemment. Pourquoi irais-je boire avec ses amies ? » « Ouais, ça se comprend. » « Il s'appelle Tom. Il est trop canon, et j'ai tellement envie d'aller prendre un verre avec lui à nouveau. »\tCC-BY 2.0 (France) Attribution: tatoeba.org #9821215 (DJ_Saidez) & #11726136 (Micsmithel)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Last 2 lines:')\n",
    "for line in lines[-2:]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English max: 55\n",
      "French max: 68\n",
      "English 98th percentile: 12.0\n",
      "French 98th percentile: 14.0\n"
     ]
    }
   ],
   "source": [
    "eng_sentences = []\n",
    "fra_sentences = []\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.strip().split('\\t')\n",
    "        eng_sentences.append(parts[0])\n",
    "        fra_sentences.append(parts[1])\n",
    "\n",
    "eng_lens = [len(s.split()) for s in eng_sentences]\n",
    "fra_lens = [len(s.split()) for s in fra_sentences]\n",
    "\n",
    "print(\"English max:\", np.max(eng_lens))\n",
    "print(\"French max:\", np.max(fra_lens))\n",
    "print(\"English 98th percentile:\", np.percentile(eng_lens, 98))\n",
    "print(\"French 98th percentile:\", np.percentile(fra_lens, 98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 13  # Max input (English) length\n",
    "Ty = 17  # Max output (French) length, including <sos> and <eos>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_count_sentences(input_path, max_words=50):\n",
    "    \"\"\"\n",
    "    Reads a parallel corpus file, filters sentence pairs where either the English\n",
    "    or French sentence exceeds max_words, and returns statistics.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to the dataset file (tab-separated English ↔ French).\n",
    "        max_words (int): Maximum allowed words in both English and French sentences.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - list: Filtered lines (kept sentence pairs as raw lines).\n",
    "            - int: Total number of sentence pairs read.\n",
    "            - int: Number of pairs exceeding max_words in either sentence.\n",
    "            - int: Number of sentences exceeding max_words in English.\n",
    "            - int: Number of sentences exceeding max_words in French.\n",
    "    \"\"\"\n",
    "    total_sentences = 0\n",
    "    filtered_sentences = []\n",
    "    exceeding_pairs = 0\n",
    "    english_exceeding = 0\n",
    "    french_exceeding = 0\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "        for line in infile:\n",
    "            total_sentences += 1\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 2:\n",
    "                continue  # Skip lines that don't have both English and French\n",
    "\n",
    "            eng = parts[0].strip()\n",
    "            fra = parts[1].strip()\n",
    "\n",
    "            eng_words = eng.split()\n",
    "            fra_words = fra.split()\n",
    "\n",
    "            eng_len = len(eng_words)\n",
    "            fra_len = len(fra_words)\n",
    "\n",
    "            eng_too_long = eng_len > max_words\n",
    "            fra_too_long = fra_len > max_words\n",
    "\n",
    "            if eng_too_long:\n",
    "                english_exceeding += 1\n",
    "            if fra_too_long:\n",
    "                french_exceeding += 1\n",
    "\n",
    "            if eng_too_long or fra_too_long:\n",
    "                exceeding_pairs += 1\n",
    "            else:\n",
    "                filtered_sentences.append(line.strip())\n",
    "\n",
    "    return filtered_sentences, total_sentences, exceeding_pairs, english_exceeding, french_exceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentence pairs: 237838\n",
      "Pairs exceeding limit: 6140\n",
      "English sentences exceeding limit: 2866\n",
      "French sentences exceeding limit: 5596\n",
      "Filtered and kept: 231698\n"
     ]
    }
   ],
   "source": [
    "filtered_sentences, total, too_long, eng_long, fra_long = filter_and_count_sentences(\n",
    "    \"dataset/raw/eng_fra.txt\", max_words=Tx)\n",
    "\n",
    "print(f\"Total sentence pairs: {total}\")\n",
    "print(f\"Pairs exceeding limit: {too_long}\")\n",
    "print(f\"English sentences exceeding limit: {eng_long}\")\n",
    "print(f\"French sentences exceeding limit: {fra_long}\")\n",
    "print(f\"Filtered and kept: {len(filtered_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_attributes_from_filtered_sentences(filtered_sentences_with_attributes):\n",
    "    \"\"\"\n",
    "    Takes a list of raw lines from the filtered dataset and removes the\n",
    "    attribution information, returning only the English and French sentences.\n",
    "\n",
    "    Args:\n",
    "        filtered_sentences_with_attributes (list): A list of strings,\n",
    "                                                   each containing \"Eng\\tFra\\tAttr\".\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings, each containing \"Eng\\tFra\".\n",
    "    \"\"\"\n",
    "    clean_sentences = []\n",
    "    for line in filtered_sentences_with_attributes:\n",
    "        # Split by the tab character. The first two parts are English and French.\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) >= 2:\n",
    "            # Reconstruct the line with only English and French, joining with a tab\n",
    "            clean_sentences.append(f\"{parts[0]}\\t{parts[1]}\")\n",
    "        else:\n",
    "            # Handle malformed lines if they exist, though they shouldn't if\n",
    "            # the original data is consistent.\n",
    "            print(f\"Warning: Skipping malformed line: {line.strip()}\")\n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset from: dataset/raw/eng_fra.txt\n",
      "\n",
      "--- Filtering Results ---\n",
      "Total sentences read: 237838\n",
      "Sentences exceeding 13  words: 6140\n",
      "Sentences to keep (<= 13 words): 231698\n",
      "\n",
      "--- Removing Attributes ---\n",
      "First 5 cleaned sentences:\n",
      "  1: Go.\tVa !\n",
      "  2: Go.\tMarche.\n",
      "  3: Go.\tEn route !\n",
      "  4: Go.\tBouge !\n",
      "  5: Hi.\tSalut !\n",
      "\n",
      "Total clean sentences for training: 231698\n",
      "\n",
      "Cleaned and filtered sentences saved to: dataset/processed/eng_fra_clean_filtered.txt\n"
     ]
    }
   ],
   "source": [
    "input_path = \"dataset/raw/eng_fra.txt\"\n",
    "\n",
    "# Filter sentences based on English word count ---\n",
    "print(f\"Processing dataset from: {input_path}\")\n",
    "max_words = Tx\n",
    "initial_filtered_sentences, total, too_long, eng_long, fra_long = filter_and_count_sentences(input_path, max_words=max_words)\n",
    "\n",
    "print(f\"\\n--- Filtering Results ---\")\n",
    "print(f\"Total sentences read: {total}\")\n",
    "print(f\"Sentences exceeding {max_words}  words: {too_long}\")\n",
    "print(f\"Sentences to keep (<= {max_words} words): {len(initial_filtered_sentences)}\")\n",
    "\n",
    "# Remove attribution from the filtered sentences ---\n",
    "print(f\"\\n--- Removing Attributes ---\")\n",
    "final_clean_sentences = remove_attributes_from_filtered_sentences(initial_filtered_sentences)\n",
    "\n",
    "print(f\"First 5 cleaned sentences:\")\n",
    "for i, line in enumerate(final_clean_sentences[:5]):\n",
    "    print(f\"  {i+1}: {line}\")\n",
    "\n",
    "print(f\"\\nTotal clean sentences for training: {len(final_clean_sentences)}\")\n",
    "\n",
    "output_clean_path = \"dataset/processed/eng_fra_clean_filtered.txt\"\n",
    "try:\n",
    "    with open(output_clean_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for sentence_pair in final_clean_sentences:\n",
    "            outfile.write(sentence_pair + '\\n') # Add newline back\n",
    "    print(f\"\\nCleaned and filtered sentences saved to: {output_clean_path}\")\n",
    "except IOError as e:\n",
    "    print(f\"Error saving file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go now.\\tAllez-y maintenant.'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_clean_sentences[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence pairs\n",
    "def load_cleaned_pairs(filepath):\n",
    "    with open(filepath, encoding=\"utf-8\") as f:\n",
    "        lines = f.read().strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = load_cleaned_pairs(\"dataset/processed/eng_fra_clean_filtered.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Go.  ->  Va !\n",
      "2: Go.  ->  Marche.\n",
      "3: Go.  ->  En route !\n",
      "4: Go.  ->  Bouge !\n",
      "5: Hi.  ->  Salut !\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"{i+1}: {pairs[i][0]}  ->  {pairs[i][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each sentence (split into list of words)\n",
    "def tokenize(sentences):\n",
    "    return [sentence.lower().strip().split() for sentence in sentences]\n",
    "\n",
    "# Separate English and French\n",
    "eng_sentences = [pair[0] for pair in pairs]\n",
    "fra_sentences = [pair[1] for pair in pairs]\n",
    "\n",
    "# Tokenize\n",
    "tokenized_eng = tokenize(eng_sentences)\n",
    "tokenized_fra = tokenize(fra_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['va', '!'],\n",
       " ['marche.'],\n",
       " ['en', 'route', '!'],\n",
       " ['bouge', '!'],\n",
       " ['salut', '!']]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_fra[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size: 29267\n",
      "French vocab size: 47464\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary dictionary\n",
    "def build_vocab(tokenized_sentences):\n",
    "    vocab = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "    index = 4\n",
    "    for sentence in tokenized_sentences:\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = index\n",
    "                index += 1\n",
    "    return vocab\n",
    "\n",
    "eng_vocab = build_vocab(tokenized_eng)\n",
    "fra_vocab = build_vocab(tokenized_fra)\n",
    "\n",
    "print(\"English vocab size:\", len(eng_vocab))\n",
    "print(\"French vocab size:\", len(fra_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1018"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_vocab['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample data pairs:\n",
      "  1. English: Go.  ->  French: Va !\n",
      "  2. English: Go.  ->  French: Marche.\n",
      "  3. English: Go.  ->  French: En route !\n",
      "  4. English: Go.  ->  French: Bouge !\n",
      "  5. English: Hi.  ->  French: Salut !\n",
      "\n",
      "Input vocabulary (eng_vocan):\n",
      "  Size: 29267\n",
      "  Example mapping: [('<pad>', 0), ('<sos>', 1), ('<eos>', 2), ('<unk>', 3), ('go.', 4), ('hi.', 5), ('run!', 6)]\n",
      "\n",
      "Output vocabulary (fra_vocab):\n",
      "  Size: 47464\n",
      "  Example mapping: [('<pad>', 0), ('<sos>', 1), ('<eos>', 2), ('<unk>', 3), ('va', 4), ('!', 5), ('marche.', 6)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSample data pairs:\")\n",
    "for i in range(5):\n",
    "    eng, fra = pairs[i][0], pairs[i][1]\n",
    "    print(f\"  {i+1}. English: {eng}  ->  French: {fra}\")\n",
    "\n",
    "print(\"\\nInput vocabulary (eng_vocan):\")\n",
    "print(f\"  Size: {len(eng_vocab)}\")\n",
    "print(f\"  Example mapping: {list(eng_vocab.items())[:7]}\")\n",
    "\n",
    "print(\"\\nOutput vocabulary (fra_vocab):\")\n",
    "print(f\"  Size: {len(fra_vocab)}\")\n",
    "print(f\"  Example mapping: {list(fra_vocab.items())[:7]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: ['go', 'now.']\n",
      "To indices: [20, 43]\n"
     ]
    }
   ],
   "source": [
    "# Convert a tokenized sentence to indices\n",
    "def sentence_to_indices(sentence_tokens, vocab):\n",
    "    return [vocab.get(word, vocab[\"<unk>\"]) for word in sentence_tokens]\n",
    "\n",
    "eng_example = tokenized_eng[100]\n",
    "fra_example = tokenized_fra[100]\n",
    "\n",
    "print(\"English:\", eng_example)\n",
    "print(\"To indices:\", sentence_to_indices(eng_example, eng_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(pairs, input_vocab, target_vocab, Tx, Ty):\n",
    "    \"\"\"\n",
    "    Prepares input/output sequences for seq2seq word-level training.\n",
    "\n",
    "    Arguments:\n",
    "    pairs        -- List of (input_sentence, target_sentence) tuples (strings)\n",
    "    input_vocab  -- Dict mapping input words to integer indices (e.g. eng_vocab)\n",
    "    target_vocab -- Dict mapping target words to integer indices (e.g. fra_vocab)\n",
    "    Tx           -- Fixed input sequence length (input, e.g. English)\n",
    "    Ty           -- Fixed output sequence length (output, e.g. French)\n",
    "\n",
    "    Returns:\n",
    "    X                -- np.array of shape (m, Tx), input sequences (as word indices)\n",
    "    Y                -- np.array of shape (m, Ty), output sequences (as word indices)\n",
    "    inv_input_vocab  -- Dict mapping input indices back to words\n",
    "    inv_target_vocab -- Dict mapping output indices back to words\n",
    "    \"\"\"\n",
    "    m = len(pairs)\n",
    "    X = np.zeros((m, Tx), dtype=np.int32)\n",
    "    Y = np.zeros((m, Ty), dtype=np.int32)\n",
    "\n",
    "    for i, (input_sentence, target_sentence) in enumerate(pairs):\n",
    "        # Tokenize sentences\n",
    "        input_tokens = tokenize([input_sentence])[0]\n",
    "        target_tokens = tokenize([target_sentence])[0]\n",
    "\n",
    "        # Add <sos> and <eos> to target\n",
    "        target_tokens = ['<sos>'] + target_tokens + ['<eos>']\n",
    "\n",
    "        # Convert tokens to indices\n",
    "        input_ids = sentence_to_indices(input_tokens, input_vocab)\n",
    "        target_ids = sentence_to_indices(target_tokens, target_vocab)\n",
    "\n",
    "        # Pad or truncate\n",
    "        input_ids = input_ids[:Tx] + [input_vocab[\"<pad>\"]] * max(0, Tx - len(input_ids))\n",
    "        target_ids = target_ids[:Ty] + [target_vocab[\"<pad>\"]] * max(0, Ty - len(target_ids))\n",
    "\n",
    "        X[i] = input_ids\n",
    "        Y[i] = target_ids\n",
    "\n",
    "    # Build inverse vocabularies\n",
    "    inv_input_vocab = {i: w for w, i in input_vocab.items()}\n",
    "    inv_target_vocab = {i: w for w, i in target_vocab.items()}\n",
    "\n",
    "    return X, Y, inv_input_vocab, inv_target_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 13   # max English length (based on 98th percentile)\n",
    "Ty = 17   # max French length\n",
    "input_vocab_size = len(eng_vocab)\n",
    "target_vocab_size = len(fra_vocab)\n",
    "embedding_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (231698, 13)\n",
      "Y shape: (231698, 17)\n"
     ]
    }
   ],
   "source": [
    "pairs = load_cleaned_pairs(\"dataset/processed/eng_fra_clean_filtered.txt\")\n",
    "\n",
    "X, Y, inv_eng_vocab, inv_fra_vocab = preprocess_data(pairs, eng_vocab, fra_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Concatenate, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def create_nmt_model(Tx, Ty, input_vocab_size, target_vocab_size, embedding_dim=256, lstm_units=512):\n",
    "    # ----- Encoder -----\n",
    "    encoder_inputs = Input(shape=(Tx,), name=\"encoder_inputs\")   # (m, Tx)\n",
    "    enc_embedding = Embedding(input_vocab_size, embedding_dim, mask_zero=True, name=\"encoder_embedding\")(encoder_inputs) # (m, tx, e_dim)\n",
    "    encoder_lstm = Bidirectional(LSTM(lstm_units, return_sequences=True, return_state=True), name=\"bi_encoder_lstm\")\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(enc_embedding)\n",
    "    encoder_outputs = Dense(lstm_units, activation=\"tanh\")(encoder_outputs)  # (None, Tx, 256)\n",
    "\n",
    "    # Concatenate forward and backward states\n",
    "    state_h = Concatenate()([forward_h, backward_h])  # (batch_size, lstm_units*2)\n",
    "    state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "    # ----- Decoder -----\n",
    "    decoder_inputs = Input(shape=(None,), name=\"decoder_inputs\")  # shifted target input\n",
    "    dec_embedding = Embedding(target_vocab_size, embedding_dim, mask_zero=True, name=\"decoder_embedding\")(decoder_inputs)\n",
    "\n",
    "    # Project encoder_outputs if needed for attention\n",
    "    attention = Attention(name=\"attention_layer\")  # Bahdanau uses score mechanism; you can switch to AdditiveAttention\n",
    "    context_vector = attention([dec_embedding, encoder_outputs])  # Shape: (batch, Ty-1, enc_seq_len)\n",
    "\n",
    "    # Concatenate context vector with decoder embedding\n",
    "    decoder_combined_input = Concatenate(axis=-1)([dec_embedding, context_vector])\n",
    "\n",
    "    # Decoder LSTM (unidirectional)\n",
    "    decoder_lstm = LSTM(lstm_units * 2, return_sequences=True, name=\"decoder_lstm\")\n",
    "    decoder_outputs = decoder_lstm(decoder_combined_input, initial_state=[state_h, state_c])\n",
    "\n",
    "    # Output layer\n",
    "    dense = Dense(target_vocab_size, activation='softmax', name=\"output_dense\")\n",
    "    decoder_outputs = dense(decoder_outputs)\n",
    "\n",
    "    # Build Model\n",
    "    model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_nmt_model(\n",
    "    Tx=13,\n",
    "    Ty=17,\n",
    "    input_vocab_size=len(eng_vocab),\n",
    "    target_vocab_size=len(fra_vocab),\n",
    "    embedding_dim=256,\n",
    "    lstm_units=256\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, 13)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder_embedding (Embedding)  (None, 13, 256)      7492352     ['encoder_inputs[0][0]']         \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " bi_encoder_lstm (Bidirectional  [(None, 13, 512),   1050624     ['encoder_embedding[0][0]']      \n",
      " )                               (None, 256),                                                     \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " decoder_embedding (Embedding)  (None, None, 256)    12150784    ['decoder_inputs[0][0]']         \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 13, 256)      131328      ['bi_encoder_lstm[0][0]']        \n",
      "                                                                                                  \n",
      " attention_layer (Attention)    (None, None, 256)    0           ['decoder_embedding[0][0]',      \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, None, 512)    0           ['decoder_embedding[0][0]',      \n",
      "                                                                  'attention_layer[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 512)          0           ['bi_encoder_lstm[0][1]',        \n",
      "                                                                  'bi_encoder_lstm[0][3]']        \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 512)          0           ['bi_encoder_lstm[0][2]',        \n",
      "                                                                  'bi_encoder_lstm[0][4]']        \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            (None, None, 512)    2099200     ['concatenate_7[0][0]',          \n",
      "                                                                  'concatenate_5[0][0]',          \n",
      "                                                                  'concatenate_6[0][0]']          \n",
      "                                                                                                  \n",
      " output_dense (Dense)           (None, None, 47464)  24349032    ['decoder_lstm[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 47,273,320\n",
      "Trainable params: 47,273,320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Custom loss\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.not_equal(real, 0)\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask  # Apply mask to ignore padding\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "# Metrics\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, model):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model([inp, targ[:, :-1]], training=True)  # Exclude last token of target\n",
    "        loss = loss_function(targ[:, 1:], predictions)  # Exclude first token (start token)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(targ[:, 1:], predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss.reset_state()\n",
    "        train_accuracy.reset_state()\n",
    "\n",
    "        print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
    "        for (batch, (inp, targ)) in enumerate(tqdm(dataset)):\n",
    "            train_step(inp, targ, model)\n",
    "\n",
    "        print(f'Epoch {epoch + 1} Loss: {train_loss.result():.4f}, Accuracy: {train_accuracy.result():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(X)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5000\n",
    "X_small = X[:num_samples]\n",
    "Y_small = Y[:num_samples]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_small, Y_small))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=num_samples).batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4/156 [00:00<00:04, 33.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:04<00:00, 36.64it/s]\n",
      "  3%|▎         | 4/156 [00:00<00:04, 32.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.9152, Accuracy: 0.1049\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:04<00:00, 36.71it/s]\n",
      "  3%|▎         | 4/156 [00:00<00:04, 34.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.7926, Accuracy: 0.1176\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:04<00:00, 36.64it/s]\n",
      "  3%|▎         | 4/156 [00:00<00:04, 33.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.6929, Accuracy: 0.1275\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:04<00:00, 36.57it/s]\n",
      "  3%|▎         | 4/156 [00:00<00:04, 33.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.6101, Accuracy: 0.1342\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:04<00:00, 36.72it/s]\n",
      "  3%|▎         | 4/156 [00:00<00:04, 33.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.5390, Accuracy: 0.1415\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:04<00:00, 36.71it/s]\n",
      "  3%|▎         | 4/156 [00:00<00:04, 32.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 0.4754, Accuracy: 0.1473\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:04<00:00, 36.48it/s]\n",
      "  3%|▎         | 4/156 [00:00<00:04, 33.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.4177, Accuracy: 0.1549\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:04<00:00, 36.30it/s]\n",
      "  3%|▎         | 4/156 [00:00<00:04, 33.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.3680, Accuracy: 0.1613\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:04<00:00, 36.38it/s]\n",
      "  3%|▎         | 4/156 [00:00<00:04, 33.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 0.3250, Accuracy: 0.1671\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:04<00:00, 36.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 0.2868, Accuracy: 0.1734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_dataset, epochs=10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sentence, word2idx, Tx):\n",
    "    tokens = sentence.lower().strip().split()\n",
    "    indices = [word2idx.get(word, word2idx['<unk>']) for word in tokens]\n",
    "    padded = indices + [word2idx['<pad>']] * (Tx - len(indices))\n",
    "    return padded[:Tx]\n",
    "\n",
    "def decode_sequence(sequence, idx2word):\n",
    "    words = [idx2word.get(idx, '') for idx in sequence]\n",
    "    return ' '.join([word for word in words if word not in ['<pad>', '<end>']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_translation(model, sentence, Tx, Ty, eng_word2idx, fra_word2idx, fra_idx2word):\n",
    "    input_seq = encode_sentence(sentence, eng_word2idx, Tx)\n",
    "    input_tensor = tf.convert_to_tensor([input_seq])  # shape: (1, Tx)\n",
    "\n",
    "    # Initialize decoder input with <sos> followed by zeros (padding)\n",
    "    start_token = fra_word2idx['<sos>']\n",
    "    end_token = fra_word2idx['<eos>']\n",
    "    decoder_input = tf.convert_to_tensor([[start_token] + [0] * (Ty - 1)])  # shape: (1, Ty)\n",
    "\n",
    "    translated_ids = []\n",
    "\n",
    "    for t in range(Ty - 1):\n",
    "        # Predict output\n",
    "        predictions = model([input_tensor, decoder_input])  # shape: (1, Ty, vocab_size)\n",
    "\n",
    "        # Get token at timestep t\n",
    "        predicted_id = tf.argmax(predictions[0, t]).numpy()\n",
    "        translated_ids.append(predicted_id)\n",
    "\n",
    "        if predicted_id == end_token:\n",
    "            break\n",
    "\n",
    "        # Update decoder input for next timestep\n",
    "        decoder_input = decoder_input.numpy()\n",
    "        decoder_input[0, t + 1] = predicted_id\n",
    "        decoder_input = tf.convert_to_tensor(decoder_input)\n",
    "\n",
    "    return decode_sequence(translated_ids, fra_idx2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: how are you\n",
      "French: êtes-vous ? <eos>\n"
     ]
    }
   ],
   "source": [
    "english_sentence = \"how are you\"\n",
    "translation = predict_translation(model, english_sentence, Tx, Ty, eng_vocab, fra_vocab, inv_fra_vocab)\n",
    "print(f\"English: {english_sentence}\")\n",
    "print(f\"French: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3620 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3620/3620 [02:47<00:00, 21.62it/s]\n",
      "  0%|          | 0/3620 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.8793, Accuracy: 0.1800\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 235/3620 [00:10<02:35, 21.73it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [173]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [171]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataset, epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (batch, (inp, targ)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(dataset)):\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, train_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
